_custom_modules:
    value: null
_wandb:
    value:
        cli_version: 0.19.9
        m: []
        python_version: 3.12.2
        t:
            "1":
                - 1
                - 5
                - 11
                - 49
                - 51
                - 53
                - 55
                - 71
                - 98
                - 105
            "2":
                - 1
                - 5
                - 11
                - 49
                - 51
                - 53
                - 55
                - 71
                - 98
                - 105
            "3":
                - 23
                - 55
            "4": 3.12.2
            "5": 0.19.9
            "6": 4.51.2
            "8":
                - 5
            "12": 0.19.9
            "13": linux-x86_64
auto_mapping:
    value: null
base_model_name_or_path:
    value: /teamspace/studios/this_studio/rag/src/finetune/../../models/base/Llama-3.2-3B-Instruct
batch_size_training:
    value: 4
batching_strategy:
    value: packing
bias:
    value: none
checkpoint_type:
    value: SHARDED_STATE_DICT
context_length:
    value: 4096
corda_config:
    value: null
dist_checkpoint_folder:
    value: fine-tuned
dist_checkpoint_root_folder:
    value: /teamspace/studios/this_studio/rag/src/finetune/models/fsdp
enable_fsdp:
    value: false
eva_config:
    value: null
exclude_modules:
    value: null
fan_in_fan_out:
    value: false
flop_counter:
    value: false
flop_counter_start:
    value: 3
freeze_LLM_only:
    value: false
freeze_layers:
    value: false
from_peft_checkpoint:
    value: ""
fsdp_activation_checkpointing:
    value: true
fsdp_cpu_offload:
    value: false
gamma:
    value: 0.85
gradient_accumulation_steps:
    value: 1
gradient_clipping:
    value: false
gradient_clipping_threshold:
    value: 1
hsdp:
    value: false
inference_mode:
    value: false
init_lora_weights:
    value: true
layer_replication:
    value: null
layers_pattern:
    value: null
layers_to_transform:
    value: null
lora_alpha:
    value: 32
lora_bias:
    value: false
lora_dropout:
    value: 0.05
low_cpu_fsdp:
    value: false
lr:
    value: 0.0001
max_eval_step:
    value: 0
max_train_step:
    value: 0
megatron_config:
    value: null
megatron_core:
    value: megatron.core
mixed_precision:
    value: true
model_name:
    value: /teamspace/studios/this_studio/rag/src/finetune/../../models/base/Llama-3.2-3B-Instruct
modules_to_save:
    value: null
num_epochs:
    value: 3
num_freeze_layers:
    value: 1
num_workers_dataloader:
    value: 1
one_gpu:
    value: false
optimizer:
    value: AdamW
output_dir:
    value: /teamspace/studios/this_studio/rag/src/finetune/models/peft
peft_method:
    value: lora
peft_type:
    value: LORA
profiler_dir:
    value: PATH/to/save/profiler/results
pure_bf16:
    value: false
quantization:
    value: null
r:
    value: 8
replica_group_size:
    value: 0
revision:
    value: null
run_validation:
    value: true
runtime_config:
    value:
        ephemeral_gpu_offload: false
save_metrics:
    value: true
save_model:
    value: true
save_optimizer:
    value: false
seed:
    value: 42
sharding_group_size:
    value: 0
sharding_strategy:
    value: FULL_SHARD
target_modules:
    value:
        - q_proj
        - v_proj
task_type:
    value: CAUSAL_LM
tokenizer_name:
    value: null
trainable_token_indices:
    value: null
use_dora:
    value: false
use_fast_kernels:
    value: true
use_fp16:
    value: false
use_peft:
    value: true
use_profiler:
    value: false
use_rslora:
    value: false
use_wandb:
    value: true
val_batch_size:
    value: 1
weight_decay:
    value: 0
